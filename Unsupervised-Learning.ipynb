import numpy as np
import pandas as pd
import matplotlib.pyplot as plt


def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2) ** 2))


def kmeans_clustering(dataset, k, num_iterations):
    # Initialize centroids randomly
    np.random.seed(42)
    centroids = dataset[np.random.choice(range(len(dataset)), k, replace=False)]

    # Assign clusters to each data point
    for _ in range(num_iterations):
        clusters = [[] for _ in range(k)]

        for point in dataset:
            distances = [euclidean_distance(point, centroid) for centroid in centroids]
            cluster_index = np.argmin(distances)
            clusters[cluster_index].append(point)

        # Update centroids
        for i in range(k):
            centroids[i] = np.mean(clusters[i], axis=0)

    # Return the final cluster assignments
    return clusters


def pca(dataset):
    # Normalize the dataset
    dataset = (dataset - np.mean(dataset, axis=0)) / np.std(dataset, axis=0)

    # Compute the covariance matrix
    covariance_matrix = np.cov(dataset.T)

    # Compute the eigenvectors and eigenvalues
    eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)

    # Sort the eigenvectors based on eigenvalues
    sorted_indices = np.argsort(eigenvalues)[::-1]
    sorted_eigenvalues = eigenvalues[sorted_indices]
    sorted_eigenvectors = eigenvectors[:, sorted_indices]

    # Return the first three eigenvectors and eigenvalues
    return sorted_eigenvectors[:, :3], sorted_eigenvalues[:3]


# Load the data from a CSV file
data = pd.read_csv(r"C:\Users\ASUS\Downloads\IITD-1\Iris Dataset.csv")

# Remove the "Species" column and store it for future comparison
species = data.pop("Species").to_numpy()

# Convert the DataFrame to a NumPy array
dataset = data.to_numpy()

# Perform K-Means Clustering
k = 3
num_iterations = 10
clusters = kmeans_clustering(dataset, k, num_iterations)

# Perform PCA on the dataset
principal_components, eigenvalues = pca(dataset)

# Visualization using Matplotlib
# Plot the output of the clusters
fig, axs = plt.subplots(1, 2, figsize=(12, 6))

for i, cluster in enumerate(clusters):
    cluster = np.array(cluster)
    axs[0].scatter(cluster[:, 0], cluster[:, 1], label=f"Cluster {i + 1}")

axs[0].set_xlabel("Sepal Length")
axs[0].set_ylabel("Sepal Width")
axs[0].set_title("K-Means Clustering - Output of Clusters")
axs[0].legend()

# Plot the actual species for comparison
unique_species = np.unique(species)
for species_name in unique_species:
    indices = np.where(species == species_name)
    species_data = dataset[indices]
    axs[1].scatter(species_data[:, 0], species_data[:, 1], label=species_name)

axs[1].set_xlabel("Sepal Length")
axs[1].set_ylabel("Sepal Width")
axs[1].set_title("K-Means Clustering - Actual Species")
axs[1].legend()

plt.tight_layout()
plt.show()

# Print the eigenvalues
print("Eigenvalues:")
print(eigenvalues)

# Visualization using Matplotlib
# Plot the first three eigenvectors
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.scatter(principal_components[:, 0], principal_components[:, 1], principal_components[:, 2])

ax.set_xlabel("PC1")
ax.set_ylabel("PC2")
ax.set_zlabel("PC3")
ax.set_title("PCA - First Three Eigenvectors")

plt.show()
